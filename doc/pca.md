# PCA

The PCA block performs dimensionality reduction using Principal Component Analysis. This algorithm enables the transformation of the current feature space into a reduced dimension space that captures most of the variance in the original data. This is done through the process called eigendecomposition where the eigenvalues and their corresponding eigenvectors are extracted from an n x f matrix (where n is the number of samples, f is the number of original features) into three matrices. You can refer to the [`pca_reference.py`](pca_reference.py) file for the implementation of the whole PCA chain in Python.

For this project, the eigendecomposition procedure will be done offline (through some software such as MATLAB, Python), and only the translation from the original feature space to the reduced dimension space will be performed. This translation is achieved through a simple dot product: the input space is a 1 x f matrix and the transformation matrix is a f x p matrix, where p is the number of reduced dimensions (corresponding to the number of principal components selected). After performing the dot product, we end up with a 1 x p matrix, the input vector in the reduced dimension space, which will then be fed to the SVM classifier.

